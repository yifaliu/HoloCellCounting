{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Using UNet to segment and count the cell from hologram images directly <br>\n",
    "The training data is from hologram obtained under 40X magnification only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0) Parameter Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function,division\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class Config(object):\n",
    "    \"\"\"Base configuration class. For custom configurations, create a\n",
    "    sub-class that inherits from this one and override properties\n",
    "    that need to be changed.\n",
    "    \"\"\" \n",
    "    # Image folder including ground truth and hologram images\n",
    "    IMG_FOLDER = \"C:/Users/yifal/Desktop/github/Holo_Cell_Counting/data/data40x/train\"\n",
    "    \n",
    "    # Image size used for algorithm\n",
    "    OUTPUT_SHAPE = (384,384)   #768\n",
    "    \n",
    "    # Base learning rate\n",
    "    BASE_LR = 0.001\n",
    "    \n",
    "    # Batch size\n",
    "    BATCH_SIZE = 4\n",
    "    \n",
    "    # Momentumn \n",
    "    MOMENTUM = 0.9\n",
    "    \n",
    "    # Weight decay\n",
    "    WEIGH_DECAY = 0.001\n",
    "    \n",
    "    # Epoch number\n",
    "    NUM_EPOCHES = 50\n",
    "    \n",
    "    # Num of GPU to use\n",
    "    GPU_COUNT = 1\n",
    "    \n",
    "    # Use the specific GPU\n",
    "    WHICH_GPU = 0\n",
    "    \n",
    "    # Num of channel\n",
    "    NUM_CHANNEL = 8\n",
    "    \n",
    "    # Num of classes to be predicted\n",
    "    NUM_CLASSES = 2\n",
    "    \n",
    "    # Folder to save the trained model\n",
    "    MODEL_DIR = \"\"\n",
    "    \n",
    "    # Folder to save the prediction results\n",
    "    RESULTS_DIR = \"\"\n",
    "    \n",
    "    # File of the pretrained model\n",
    "    PRETRAIN_FILE = \"\"\n",
    "    \n",
    "    \n",
    "    # Gaussin smoothing\n",
    "    HAS_GAUSSIN_SMOOTHING = True\n",
    "    \n",
    "    # Resume Point\n",
    "    RESUME_POINT = 0\n",
    "    \n",
    "    # num of workers used for data loading\n",
    "    NUM_WORKERS = 4\n",
    "\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"Nonthing\"\n",
    "        \n",
    "    def display(self):\n",
    "        \"\"\"Display Configuration values.\"\"\"\n",
    "        print(\"\\nConfigurations:\")\n",
    "        for a in dir(self):\n",
    "            if not a.startswith(\"__\") and not callable(getattr(self, a)):\n",
    "                print(\"{:30} {}\".format(a, getattr(self, a)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1)UNet Algorithm Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function,division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"\n",
    "    unet algorithm\n",
    "    \"\"\"\n",
    "    def __init__(self,inputs,labels):\n",
    "        super(UNet,self).__init__()\n",
    "        \n",
    "        self.inputs = inputs\n",
    "        \n",
    "        # encode1\n",
    "        self.conv11 = nn.Conv2d(inputs, 64, kernel_size=3, padding=1)\n",
    "        self.bn11 = nn.BatchNorm2d(64)\n",
    "        self.conv12 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn12 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        # encode2\n",
    "        self.conv21 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn21 = nn.BatchNorm2d(128)\n",
    "        self.conv22 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.bn22 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        # encode3\n",
    "        self.conv31 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn31 = nn.BatchNorm2d(256)\n",
    "        self.conv32 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.bn32 = nn.BatchNorm2d(256)\n",
    "        self.conv33 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.bn33 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        # encode4\n",
    "        self.conv41 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.bn41 = nn.BatchNorm2d(512)\n",
    "        self.conv42 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.bn42 = nn.BatchNorm2d(512)\n",
    "        self.conv43 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.bn43 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        # encode5\n",
    "        self.conv51 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.bn51 = nn.BatchNorm2d(512)\n",
    "        self.conv52 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.bn52 = nn.BatchNorm2d(512)\n",
    "        self.conv53 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.bn53 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        #=============================>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "        \n",
    "        # decode 5\n",
    "        self.upconv5 = nn.ConvTranspose2d(512, 512, kernel_size=3, padding=1, stride=2, output_padding=1)\n",
    "\n",
    "        self.conv53d = nn.Conv2d(1024, 512, kernel_size=3, padding=1)\n",
    "        self.bn53d = nn.BatchNorm2d(512)\n",
    "        self.conv52d = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.bn52d = nn.BatchNorm2d(512)\n",
    "        self.conv51d = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.bn51d = nn.BatchNorm2d(512)\n",
    "        \n",
    "        # decode 4\n",
    "        self.upconv4 = nn.ConvTranspose2d(512, 512, kernel_size=3, padding=1, stride=2, output_padding=1)\n",
    "\n",
    "        self.conv43d = nn.Conv2d(1024, 512, kernel_size=3, padding=1)\n",
    "        self.bn43d = nn.BatchNorm2d(512)\n",
    "        self.conv42d = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.bn42d = nn.BatchNorm2d(512)\n",
    "        self.conv41d = nn.Conv2d(512, 256, kernel_size=3, padding=1)\n",
    "        self.bn41d = nn.BatchNorm2d(256)\n",
    "        \n",
    "        # decode 3\n",
    "        self.upconv3 = nn.ConvTranspose2d(256, 256, kernel_size=3, padding=1, stride=2, output_padding=1)\n",
    "\n",
    "        self.conv33d = nn.Conv2d(512, 256, kernel_size=3, padding=1)\n",
    "        self.bn33d = nn.BatchNorm2d(256)\n",
    "        self.conv32d = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.bn32d = nn.BatchNorm2d(256)\n",
    "        self.conv31d = nn.Conv2d(256,  128, kernel_size=3, padding=1)\n",
    "        self.bn31d = nn.BatchNorm2d(128)\n",
    "        \n",
    "        # decode 2\n",
    "        self.upconv2 = nn.ConvTranspose2d(128, 128, kernel_size=3, padding=1, stride=2, output_padding=1)\n",
    "\n",
    "        self.conv22d = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
    "        self.bn22d = nn.BatchNorm2d(128)\n",
    "        self.conv21d = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
    "        self.bn21d = nn.BatchNorm2d(64)\n",
    "        \n",
    "        # decode 1\n",
    "        self.upconv1 = nn.ConvTranspose2d(64, 64, kernel_size=3, padding=1, stride=2, output_padding=1)\n",
    "\n",
    "        self.conv12d = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
    "        self.bn12d = nn.BatchNorm2d(64)\n",
    "        self.conv11d = nn.Conv2d(64, labels, kernel_size=3, padding=1)\n",
    "        \n",
    "    \n",
    "    def forward(self,x):\n",
    "        # encode 1\n",
    "        x11 = F.relu(self.bn11(self.conv11(x)))\n",
    "        x12 = F.relu(self.bn12(self.conv12(x11)))\n",
    "        x1p = F.max_pool2d(x12, kernel_size=2, stride=2)\n",
    "        \n",
    "        # encode 2\n",
    "        x21 = F.relu(self.bn21(self.conv21(x1p)))\n",
    "        x22 = F.relu(self.bn22(self.conv22(x21)))\n",
    "        x2p = F.max_pool2d(x22, kernel_size=2, stride=2)\n",
    "        \n",
    "        # encode 3\n",
    "        x31 = F.relu(self.bn31(self.conv31(x2p)))\n",
    "        x32 = F.relu(self.bn32(self.conv32(x31)))\n",
    "        x33 = F.relu(self.bn33(self.conv33(x32)))\n",
    "        x3p = F.max_pool2d(x33, kernel_size=2, stride=2)\n",
    "        \n",
    "        # encode 4\n",
    "        x41 = F.relu(self.bn41(self.conv41(x3p)))\n",
    "        x42 = F.relu(self.bn42(self.conv42(x41)))\n",
    "        x43 = F.relu(self.bn43(self.conv43(x42)))\n",
    "        x4p = F.max_pool2d(x43, kernel_size=2, stride=2)\n",
    "        \n",
    "        # encode 5\n",
    "        x51 = F.relu(self.bn51(self.conv51(x4p)))\n",
    "        x52 = F.relu(self.bn52(self.conv52(x51)))\n",
    "        x53 = F.relu(self.bn53(self.conv53(x52)))\n",
    "        x5p = F.max_pool2d(x53, kernel_size=2, stride=2)\n",
    "        \n",
    "        # decode 5\n",
    "        x5d = torch.cat((self.upconv5(x5p), x53), 1)\n",
    "        x53d = F.relu(self.bn53d(self.conv53d(x5d)))\n",
    "        x52d = F.relu(self.bn52d(self.conv52d(x53d)))\n",
    "        x51d = F.relu(self.bn51d(self.conv51d(x52d)))\n",
    "        \n",
    "        # decode 4\n",
    "        x4d = torch.cat((self.upconv4(x51d), x43), 1)\n",
    "        x43d = F.relu(self.bn43d(self.conv43d(x4d)))\n",
    "        x42d = F.relu(self.bn42d(self.conv42d(x43d)))\n",
    "        x41d = F.relu(self.bn41d(self.conv41d(x42d)))\n",
    "        \n",
    "        # decode 3\n",
    "        x3d = torch.cat((self.upconv3(x41d), x33), 1)\n",
    "        x33d = F.relu(self.bn33d(self.conv33d(x3d)))\n",
    "        x32d = F.relu(self.bn32d(self.conv32d(x33d)))\n",
    "        x31d = F.relu(self.bn31d(self.conv31d(x32d)))\n",
    "        \n",
    "        # decode 2\n",
    "        x2d = torch.cat((self.upconv2(x31d), x22), 1)\n",
    "        x22d = F.relu(self.bn22d(self.conv22d(x2d)))\n",
    "        x21d = F.relu(self.bn21d(self.conv21d(x22d)))\n",
    "        \n",
    "        # decode 1\n",
    "        x1d = torch.cat((self.upconv1(x21d), x12), 1)\n",
    "        x12d = F.relu(self.bn12d(self.conv12d(x1d)))\n",
    "        x11d = self.conv11d(x12d)\n",
    "        \n",
    "        return x11d\n",
    "    \n",
    "    \n",
    "    def initialized_with_pretrained_weights(self):\n",
    "        \"\"\"Initialiaze.\"\"\"\n",
    "        corresp_name = {\n",
    "                        \"features.0.weight\": \"conv11.weight\",\n",
    "                        \"features.0.bias\": \"conv11.bias\",\n",
    "                        \"features.1.weight\": \"bn11.weight\",\n",
    "                        \"features.1.bias\": \"bn11.bias\",\n",
    "                        \"features.1.running_mean\": \"bn11.running_mean\",\n",
    "                        \"features.1.running_var\": \"bn11.running_var\",\n",
    "\n",
    "                        \"features.3.weight\": \"conv12.weight\",\n",
    "                        \"features.3.bias\": \"conv12.bias\",\n",
    "                        \"features.4.weight\": \"bn12.weight\",\n",
    "                        \"features.4.bias\": \"bn12.bias\",\n",
    "                        \"features.4.running_mean\": \"bn12.running_mean\",\n",
    "                        \"features.4.running_var\": \"bn12.running_var\",\n",
    "\n",
    "                        \"features.7.weight\": \"conv21.weight\",\n",
    "                        \"features.7.bias\": \"conv21.bias\",\n",
    "                        \"features.8.weight\": \"bn21.weight\",\n",
    "                        \"features.8.bias\": \"bn21.bias\",\n",
    "                        \"features.8.running_mean\": \"bn21.running_mean\",\n",
    "                        \"features.8.running_var\": \"bn21.running_var\",\n",
    "\n",
    "                        \"features.10.weight\": \"conv22.weight\",\n",
    "                        \"features.10.bias\": \"conv22.bias\",\n",
    "                        \"features.11.weight\": \"bn22.weight\",\n",
    "                        \"features.11.bias\": \"bn22.bias\",\n",
    "                        \"features.11.running_mean\": \"bn22.running_mean\",\n",
    "                        \"features.11.running_var\": \"bn22.running_var\",\n",
    "\n",
    "                        # stage 3\n",
    "                        \"features.14.weight\": \"conv31.weight\",\n",
    "                        \"features.14.bias\": \"conv31.bias\",\n",
    "                        \"features.15.weight\": \"bn31.weight\",\n",
    "                        \"features.15.bias\": \"bn31.bias\",\n",
    "                        \"features.15.running_mean\": \"bn31.running_mean\",\n",
    "                        \"features.15.running_var\": \"bn31.running_var\",\n",
    "\n",
    "                        \"features.17.weight\": \"conv32.weight\",\n",
    "                        \"features.17.bias\": \"conv32.bias\",\n",
    "                        \"features.18.weight\": \"bn32.weight\",\n",
    "                        \"features.18.bias\": \"bn32.bias\",\n",
    "                        \"features.18.running_mean\": \"bn32.running_mean\",\n",
    "                        \"features.18.running_var\": \"bn32.running_var\",\n",
    "\n",
    "                        \"features.20.weight\": \"conv33.weight\",\n",
    "                        \"features.20.bias\": \"conv33.bias\",\n",
    "                        \"features.21.weight\": \"bn33.weight\",\n",
    "                        \"features.21.bias\": \"bn33.bias\",\n",
    "                        \"features.21.running_mean\": \"bn33.running_mean\",\n",
    "                        \"features.21.running_var\": \"bn33.running_var\",\n",
    "\n",
    "                        # stage 4\n",
    "                        \"features.24.weight\": \"conv41.weight\",\n",
    "                        \"features.24.bias\": \"conv41.bias\",\n",
    "                        \"features.25.weight\": \"bn41.weight\",\n",
    "                        \"features.25.bias\": \"bn41.bias\",\n",
    "                        \"features.25.running_mean\": \"bn41.running_mean\",\n",
    "                        \"features.25.running_var\": \"bn41.running_var\",\n",
    "\n",
    "                        \"features.27.weight\": \"conv42.weight\",\n",
    "                        \"features.27.bias\": \"conv42.bias\",\n",
    "                        \"features.28.weight\": \"bn42.weight\",\n",
    "                        \"features.28.bias\": \"bn42.bias\",\n",
    "                        \"features.28.running_mean\": \"bn42.running_mean\",\n",
    "                        \"features.28.running_var\": \"bn42.running_var\",\n",
    "\n",
    "                        \"features.30.weight\": \"conv43.weight\",\n",
    "                        \"features.30.bias\": \"conv43.bias\",\n",
    "                        \"features.31.weight\": \"bn43.weight\",\n",
    "                        \"features.31.bias\": \"bn43.bias\",\n",
    "                        \"features.31.running_mean\": \"bn43.running_mean\",\n",
    "                        \"features.31.running_var\": \"bn43.running_var\",\n",
    "\n",
    "                        # stage 5\n",
    "                        \"features.34.weight\": \"conv51.weight\",\n",
    "                        \"features.34.bias\": \"conv51.bias\",\n",
    "                        \"features.35.weight\": \"bn51.weight\",\n",
    "                        \"features.35.bias\": \"bn51.bias\",\n",
    "                        \"features.35.running_mean\": \"bn51.running_mean\",\n",
    "                        \"features.35.running_var\": \"bn51.running_var\",\n",
    "\n",
    "                        \"features.37.weight\": \"conv52.weight\",\n",
    "                        \"features.37.bias\": \"conv52.bias\",\n",
    "                        \"features.38.weight\": \"bn52.weight\",\n",
    "                        \"features.38.bias\": \"bn52.bias\",\n",
    "                        \"features.38.running_mean\": \"bn52.running_mean\",\n",
    "                        \"features.38.running_var\": \"bn52.running_var\",\n",
    "\n",
    "                        \"features.40.weight\": \"conv53.weight\",\n",
    "                        \"features.40.bias\": \"conv53.bias\",\n",
    "                        \"features.41.weight\": \"bn53.weight\",\n",
    "                        \"features.41.bias\": \"bn53.bias\",\n",
    "                        \"features.41.running_mean\": \"bn53.running_mean\",\n",
    "                        \"features.41.running_var\": \"bn53.running_var\",\n",
    "                        }\n",
    "        # load the state dict of pretrained model\n",
    "        import torch.utils.model_zoo as model_zoo\n",
    "        pretrained_sd = model_zoo.load_url(\"http://download.pytorch.org/models/vgg16_bn-6c64b313.pth\")\n",
    "        s_dict = self.state_dict()\n",
    "        for name in pretrained_sd:\n",
    "            if name not in corresp_name:\n",
    "                continue\n",
    "            if(\"features.0\" not in name) or (self.inputs==3):\n",
    "                s_dict[corresp_name[name]] = pretrained_sd[name]\n",
    "        self.load_state_dict(s_dict)\n",
    "        \n",
    "    \n",
    "    def load_from_filename(self,model_path):\n",
    "        \"\"\"Load weights from filename.\"\"\"\n",
    "        th = torch.load(model_path)  # .state_dict()  # load the weigths\n",
    "        self.load_state_dict(th)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import numpy as np\n",
    "import cv2\n",
    "import skimage.io\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io,transform\n",
    "import skimage\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torchvision import transforms,utils\n",
    "from PIL import Image\n",
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "import random\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "import os\n",
    "\n",
    "\n",
    "class HoloDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Images from Hologram Dataset\n",
    "    \"\"\"\n",
    "    def __init__(self,config,avg,std,has_augment=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            config: has all related parameters\n",
    "            indicator: \"train\",\"val\" or \"test\"\n",
    "            avg,std: average and standard deviation value for the training dataset\n",
    "            transform(callable,optional): image transform\n",
    "        \"\"\"\n",
    "        self.image_folder = config.IMG_FOLDER\n",
    "        self.output_shape = config.OUTPUT_SHAPE\n",
    "        self.has_augment = has_augment\n",
    "        self.avg,self.std = avg,std\n",
    "        self.has_gaussian_smoothing = config.HAS_GAUSSIN_SMOOTHING\n",
    "        self.mask_imgs = glob(self.image_folder + os.sep + 'Mask_*')\n",
    "            \n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        length of the dataset\n",
    "        \"\"\"\n",
    "        return len(self.mask_imgs)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        \"\"\"\n",
    "        get one element from dataset with index idx\n",
    "        \"\"\"\n",
    "\n",
    "        # mask image\n",
    "        gt_img = self.mask_imgs[idx]\n",
    "        mask = skimage.io.imread(gt_img)\n",
    "        mask = mask / 255\n",
    "        \n",
    "        # hologram image\n",
    "        holo_img = gt_img.replace('Mask','Hologram')\n",
    "        img = skimage.io.imread(holo_img)\n",
    "        \n",
    "        # resize the image\n",
    "        output_shape = (self.output_shape[1],self.output_shape[0])\n",
    "        img = cv2.resize(img,output_shape,interpolation = cv2.INTER_NEAREST)\n",
    "        mask = cv2.resize(mask,output_shape,interpolation = cv2.INTER_NEAREST)\n",
    "        \n",
    "        \n",
    "        # normalization\n",
    "        img = (img - self.avg) / self.std\n",
    "        \n",
    "        # smoothing\n",
    "        if self.has_gaussian_smoothing:\n",
    "            img = gaussian_filter(img,sigma=3)\n",
    "        \n",
    "        \n",
    "        img = img[np.newaxis,...]\n",
    "        mask = mask[np.newaxis,...]\n",
    "        \n",
    "        sample = {'image': img.astype(np.float),'mask':mask.astype(np.float)}\n",
    "        \n",
    "        if self.has_augment:\n",
    "            sample = self.has_augment(sample)\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ImgAugmentation(object):\n",
    "    \"\"\"image augmentation definition\"\"\"\n",
    "    def __init__(self):\n",
    "        self.seq = iaa.SomeOf(2,[\n",
    "            iaa.Affine(rotate=90,order=[0]),\n",
    "            iaa.Affine(translate_percent={\"x\": (-0.1, 0.1), \"y\": (-0.1, 0.1)}),\n",
    "            iaa.Affine(rotate=(-45, 45)),\n",
    "            iaa.Fliplr(0.5),\n",
    "            iaa.Flipud(0.5),\n",
    "            ])\n",
    "        \n",
    "    def __call__(self,sample):\n",
    "        \"\"\"\n",
    "        shape for image augmentation must be: (height, width, channels) or (N, height, width, channels)\n",
    "        \"\"\"\n",
    "        \n",
    "        if np.random.uniform(0,1) > 1.0/50:\n",
    "            image = sample['image'].transpose((1,2,0))\n",
    "            mask = sample['mask'].transpose((1,2,0))\n",
    "            \n",
    "            seed = np.random.randint(10000)\n",
    "            ia.seed(seed)\n",
    "            image = self.seq.augment_image(image)\n",
    "            ia.seed(seed)\n",
    "            mask = self.seq.augment_image(mask)\n",
    "            \n",
    "            image = image.transpose((2,0,1))   # to (channels height width)\n",
    "            mask = mask.transpose((2,0,1))     # to (channels height width)\n",
    "            \n",
    "            return {'image':image,'mask':mask}\n",
    "        else:\n",
    "            return sample\n",
    "        \n",
    "        \n",
    "class ToTensor(object):\n",
    "    \"\"\"\n",
    "    convert ndarrays in sample to Tensors\n",
    "    \"\"\"\n",
    "    def __call__(self,sample):\n",
    "        image,mask = sample['image'],sample['mask']\n",
    "        \n",
    "        return {'image':torch.from_numpy(image.astype(np.float)),\n",
    "               'mask':torch.from_numpy(mask.astype(np.float))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "def calculate_avg_std(img_folder):\n",
    "    \"\"\"\n",
    "    calculate the average and stdard deviation\n",
    "    \"\"\"\n",
    "    avg = []\n",
    "    std = []\n",
    "    imgs = glob(img_folder + os.sep + 'Hologram*.tif')\n",
    "    for ele in imgs:\n",
    "        img = skimage.io.imread(ele)\n",
    "        avg.append(np.mean(img))\n",
    "        std.append(np.std(img))\n",
    "    \n",
    "    return np.mean(avg),np.mean(std)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(true_labels,pred_labels,row,col,batch_size,num_classes):\n",
    "    accuracy = []\n",
    "    accuracy.append(float(np.sum(true_labels == pred_labels)) / (row * col * batch_size))\n",
    "    for n_label in range(num_classes):\n",
    "        n_target = np.sum(true_labels == n_label)\n",
    "        if n_target > 0:\n",
    "            accuracy.append(float(np.sum((true_labels == n_label) & (pred_labels == n_label))) / n_target)\n",
    "        else:\n",
    "            accuracy.append(np.nan)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Training and Inference Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.backends.cudnn as cudnn\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "\n",
    "def train(config,net,holo_dataset,holo_val_dataset,shuffle=True):\n",
    "    \"\"\"\n",
    "    Train network based on training data\n",
    "    \"\"\"\n",
    "    \n",
    "    LR_OPT = config.BASE_LR\n",
    "    WEIGHT_DECAY_OPT = config.WEIGH_DECAY\n",
    "    \n",
    "    # Set CUDA mode\n",
    "    if torch.cuda.is_available() & config.GPU_COUNT:\n",
    "        device = torch.device(\"cuda\", index=config.WHICH_GPU)\n",
    "    else:\n",
    "        print(\"Using CPU to train the model!\")\n",
    "        device = torch.device(\"cpu\")\n",
    "    \n",
    "    # initialization\n",
    "    net.initialized_with_pretrained_weights()\n",
    "\n",
    "    optimizer = optim.SGD(net.parameters(),lr=LR_OPT,momentum=config.MOMENTUM,weight_decay=WEIGHT_DECAY_OPT)\n",
    "    \n",
    "    class_weights = np.array([0.50,2.50])\n",
    "    class_weights = torch.FloatTensor(class_weights).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)  \n",
    "    \n",
    "    loss_history,val_loss = [],[]\n",
    "    accuracy_history,val_accuracy = [],[]\n",
    "    dice_history,val_dice_score = [],[]\n",
    "    smallest_loss = 10000\n",
    "    for epoch in range(config.NUM_EPOCHES):\n",
    "        train_dataloader = DataLoader(holo_dataset,batch_size=config.BATCH_SIZE,shuffle=shuffle,num_workers=config.NUM_WORKERS)\n",
    "        \n",
    "        epoch_loss = []\n",
    "        epoch_accuracy = []\n",
    "        epoch_dice = []\n",
    "        for i_batch, sample_batched in enumerate(train_dataloader):\n",
    "            images_batch,mask_batch = sample_batched['image'],sample_batched['mask']\n",
    "            images_batch = images_batch.float()   #float()\n",
    "            mask_batch = mask_batch.long()\n",
    "            \n",
    "            images = images_batch.to(device)\n",
    "            masks = mask_batch.to(device)\n",
    "            \n",
    "            net.train()\n",
    "            pred = net(images)\n",
    "            \n",
    "            # for CrossEntropyLoss\n",
    "            loss = criterion(pred,torch.squeeze(masks,1))\n",
    "            epoch_loss.append(loss.item())\n",
    "\n",
    "            pred_labels = F.softmax(pred).cpu().detach().numpy()\n",
    "            pred_labels = np.argmax(pred_labels, axis=1).astype('int')  # [batch_size, h, w]\n",
    "            true_labels = masks.cpu().numpy()\n",
    "            true_labels = np.squeeze(true_labels,1)\n",
    "            \n",
    "            # Accuracy\n",
    "            accuracy = calculate_accuracy(true_labels,pred_labels,config.OUTPUT_SHAPE[0],config.OUTPUT_SHAPE[1],config.BATCH_SIZE,config.NUM_CLASSES)\n",
    "            epoch_accuracy.append(accuracy)\n",
    "            \n",
    "            # dice score\n",
    "            dice_score = (2.0 * np.sum(true_labels * pred_labels)) / (np.sum(true_labels) + np.sum(pred_labels))\n",
    "            epoch_dice.append(dice_score)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        print(\"Training: Epoch {}, loss: {}, accuracy: {}, Dice Score: {}\".format(epoch + 1, np.average(epoch_loss), np.average(epoch_accuracy, axis=0),np.average(epoch_dice))) \n",
    "        loss_history.append(np.average(epoch_loss))\n",
    "        accuracy_history.append(np.nanmean(epoch_accuracy, axis=0))\n",
    "        dice_history.append(np.nanmean(epoch_dice))\n",
    "        \n",
    "        ### validation\n",
    "        val_loss_epoch,val_accuracy_epoch,val_dice_score_epoch = val_infer(config,net,holo_val_dataset,criterion)\n",
    "        print(\"Validation: Epoch {}, loss: {}, accuracy: {}, Dice Score: {}\".format(epoch + 1, val_loss_epoch, val_accuracy_epoch,val_dice_score_epoch))\n",
    "        val_loss.append(val_loss_epoch)\n",
    "        val_accuracy.append(val_accuracy_epoch)\n",
    "        val_dice_score.append(val_dice_score_epoch)\n",
    "        \n",
    "        \n",
    "        # save trained model (smallest loss)\n",
    "        save_model_to = config.MODEL_DIR\n",
    "        if not os.path.exists(save_model_to):\n",
    "            os.makedirs(save_model_to)\n",
    "        \n",
    "        if val_loss_epoch < smallest_loss:\n",
    "            torch.save(net.state_dict(),save_model_to + os.sep + 'UNet.pth')\n",
    "            smallest_loss = val_loss_epoch\n",
    "            print('The best epoch:{}'.format(epoch))\n",
    "     \n",
    "    return loss_history,accuracy_history,dice_history,val_loss,val_accuracy,val_dice_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Inference function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "import pandas_ml as pdml\n",
    "from timeit import default_timer as timer\n",
    "import scipy.misc\n",
    "\n",
    "def val_infer(config,net,holo_dataset,criterion):\n",
    "    \"\"\"\n",
    "    do inference\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set CUDA mode\n",
    "    if torch.cuda.is_available() & config.GPU_COUNT:\n",
    "        device = torch.device(\"cuda\", index=config.WHICH_GPU)\n",
    "    else:\n",
    "        print(\"Using CPU to train the model!\")\n",
    "        device = torch.device(\"cpu\")\n",
    "        \n",
    "    test_time = []\n",
    "    # testing the model\n",
    "    test_dataloader = DataLoader(holo_dataset,batch_size=config.BATCH_SIZE,shuffle=False,num_workers=config.NUM_WORKERS)\n",
    "    \n",
    "    epoch_loss = []\n",
    "    epoch_accuracy = []\n",
    "    epoch_dice = []\n",
    "    for i_batch,sample_batched in enumerate(test_dataloader):\n",
    "        start_time_test = timer()\n",
    "        images_batch,mask_batch = sample_batched['image'],sample_batched['mask']\n",
    "        images_batch = images_batch.float()\n",
    "        mask_batch = mask_batch.long()\n",
    "        \n",
    "        images = images_batch.to(device)\n",
    "        masks = mask_batch.to(device)\n",
    "            \n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            pred = net(images)\n",
    "            \n",
    "        val_loss = criterion(pred,torch.squeeze(masks,1))\n",
    "        epoch_loss.append(val_loss.item())\n",
    "        \n",
    "        pred_labels = F.softmax(pred).cpu().detach().numpy()\n",
    "        pred_labels = np.argmax(pred_labels, axis=1).astype('int')  # [batch_size, h, w]\n",
    "        true_labels = masks.cpu().numpy()\n",
    "        true_labels = np.squeeze(true_labels,1)\n",
    "\n",
    "        # Accuracy\n",
    "        accuracy =  calculate_accuracy(true_labels,\n",
    "                                       pred_labels,\n",
    "                                       config.OUTPUT_SHAPE[0],\n",
    "                                       config.OUTPUT_SHAPE[1],\n",
    "                                       config.BATCH_SIZE,\n",
    "                                       config.NUM_CLASSES)    \n",
    "        \n",
    "        epoch_accuracy.append(accuracy)\n",
    "\n",
    "        # dice score\n",
    "        dice_score = (2.0 * np.sum(true_labels * pred_labels)) / (np.sum(true_labels) + np.sum(pred_labels))\n",
    "        epoch_dice.append(dice_score)\n",
    "        \n",
    "    return np.average(epoch_loss),np.nanmean(epoch_accuracy, axis=0),np.nanmean(epoch_dice)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "import pandas_ml as pdml\n",
    "from timeit import default_timer as timer\n",
    "import scipy.misc\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.morphology import binary_opening\n",
    "from skimage.morphology import disk\n",
    "from skimage.feature import peak_local_max\n",
    "from skimage import measure\n",
    "from skimage.morphology import dilation\n",
    "from skimage.measure import regionprops\n",
    "\n",
    "def infer(config,net,holo_test_dataset,save_results_to):\n",
    "    \"\"\"\n",
    "    do inference\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set CUDA mode\n",
    "    if torch.cuda.is_available() & config.GPU_COUNT:\n",
    "        device = torch.device(\"cuda\", index=config.WHICH_GPU)\n",
    "    else:\n",
    "        print(\"Using CPU to train the model!\")\n",
    "        device = torch.device(\"cpu\")\n",
    "        \n",
    "    test_time = []\n",
    "    # testing the model\n",
    "    test_dataloader = DataLoader(holo_test_dataset,batch_size=config.BATCH_SIZE,shuffle=False,num_workers=config.NUM_WORKERS)\n",
    "    \n",
    "    indexTmp = 0\n",
    "    test_accuracy,test_dice = [],[]\n",
    "    cell_num_pred,cell_num_pred_corr,cell_num_true = [],[],[]\n",
    "    for i_batch,sample_batched in enumerate(test_dataloader):\n",
    "        start_time_test = timer()\n",
    "        images_batch,mask_batch = sample_batched['image'],sample_batched['mask']\n",
    "        images_batch = images_batch.float()\n",
    "        mask_batch = mask_batch.float()\n",
    "        \n",
    "        images = images_batch.to(device)\n",
    "        masks = mask_batch.to(device)\n",
    "            \n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            output = net(images)\n",
    "            \n",
    "        pred = nn.Softmax2d()(output)\n",
    "        pred = pred.data.cpu().numpy()\n",
    "        end_time_test = timer()\n",
    "        test_time = test_time + [end_time_test - start_time_test]\n",
    "        \n",
    "        \n",
    "        # show image\n",
    "        num = pred.shape[0]\n",
    "        for i in range(num):\n",
    "            oriImg = images_batch.numpy()[i,0,:,:]\n",
    "            print(type(oriImg[0][0]))\n",
    "            oriGT = mask_batch.numpy()[i,0,:,:]\n",
    "            oriGT = scipy.ndimage.binary_opening(oriGT, structure=disk(3)).astype(np.int)\n",
    "            prob = pred[i,1,:,:]\n",
    "            predImg = 1 * (pred[i,1,:,:] > 0.5)\n",
    "            predImg2 = np.copy(predImg)\n",
    "            predImg = predImg.astype('uint8')\n",
    "            \n",
    "            \n",
    "            # Accuracy\n",
    "            accuracy =  calculate_accuracy(oriGT,\n",
    "                                           predImg,\n",
    "                                           config.OUTPUT_SHAPE[0],\n",
    "                                           config.OUTPUT_SHAPE[1],\n",
    "                                           1,\n",
    "                                           config.NUM_CLASSES)  \n",
    "\n",
    "            test_accuracy.append(accuracy)\n",
    "\n",
    "            # dice score\n",
    "            dice_score = (2.0 * np.sum(oriGT * predImg)) / (np.sum(oriGT) + np.sum(predImg))\n",
    "            test_dice.append(dice_score)\n",
    "            \n",
    "            # save images\n",
    "            scipy.misc.imsave(save_results_to + '/raw_' + str(indexTmp) + '.tif',oriImg)\n",
    "            scipy.misc.imsave(save_results_to + '/prediction_' + str(indexTmp) + '.tif', predImg.astype(float))\n",
    "            \n",
    "            label_mask = np.zeros_like(oriGT)\n",
    "            pixels = np.unique(oriGT)\n",
    "            \n",
    "            non_heal_index = oriGT == 1\n",
    "            label_mask[non_heal_index] = 255   #non-healing: 255\n",
    "            \n",
    "            label_mask = np.uint8(label_mask)\n",
    "            scipy.misc.toimage(label_mask.astype(float),cmin=0.0,cmax=255.0).save(save_results_to + \"/mask_\" + str(indexTmp) + '.tif')\n",
    "\n",
    "            \n",
    "            # post-processing --- morphology opening\n",
    "            post_pred = predImg2 > 0.5\n",
    "            post_pred = scipy.ndimage.binary_opening(post_pred, structure=disk(5)).astype(np.uint8)\n",
    "            scipy.misc.imsave(save_results_to + '/post_prediction_' + str(indexTmp) + '.tif', post_pred.astype(float))\n",
    "            \n",
    "            \n",
    "            # detect the cell central points\n",
    "            dist_transform = cv2.distanceTransform(post_pred,cv2.DIST_L2,5)\n",
    "            coordinates = peak_local_max(dist_transform,min_distance=2)\n",
    "            \n",
    "            indexTmp = indexTmp + 1\n",
    "            \n",
    "            # show the images\n",
    "            f = plt.figure(figsize=(15,100))\n",
    "            ax1 = f.add_subplot(1,5,1)\n",
    "            plt.imshow(oriImg,cmap='gray')\n",
    "            ax2 = f.add_subplot(1,5,2)\n",
    "            plt.imshow(predImg)\n",
    "            ax3 = f.add_subplot(1,5,3)\n",
    "            plt.imshow(post_pred)\n",
    "            ax4 = f.add_subplot(1,5,4)\n",
    "            plt.imshow(oriGT)\n",
    "            ax5 = f.add_subplot(1,5,5)\n",
    "            plt.imshow(post_pred)\n",
    "            ax5.plot(coordinates[:,1],coordinates[:,0],'r.')\n",
    "\n",
    "            ax1.title.set_text('Original Image')\n",
    "            ax2.title.set_text('Predicted Image')\n",
    "            ax3.title.set_text('Morphology Opening')\n",
    "            ax4.title.set_text('Ground Truth Image')\n",
    "            ax5.title.set_text('Cell Central Points')\n",
    "\n",
    "            plt.show()\n",
    "            \n",
    "    print('inference time: {}'.format(np.mean(test_time) / num))    \n",
    "    print(\"Accuracy: {}, Dice Score:{}\".format(np.nanmean(test_accuracy,axis=0),np.nanmean(test_dice)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Start Training and Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "config = Config()\n",
    "config.BASE_LR = 0.01    #0.01 0.001 0.0001 0.00001\n",
    "config.WEIGH_DECAY = 0.0001  # 0.01 0.001 0.0001 0.00001\n",
    "config.NUM_EPOCHES = 1\n",
    "config.IMG_FOLDER = 'C:/Users/yifal/Desktop/github/Holo_Cell_Counting/data/data40x/train'\n",
    "config.MODEL_DIR = 'C:/Users/yifal/Desktop/github/Holo_Cell_Counting/results/trainedModel'\n",
    "config.RESULTS_DIR = 'C:/Users/yifal/Desktop/github/Holo_Cell_Counting/results/predictedImages'\n",
    "config.display()\n",
    "\n",
    "# calculate average and standard devation\n",
    "avg,std = calculate_avg_std(config.IMG_FOLDER)\n",
    "train_dataset = HoloDataset(config,\n",
    "                            avg,\n",
    "                            std,\n",
    "                            has_augment=transforms.Compose([ImgAugmentation(),ToTensor()])\n",
    "                            ) \n",
    "config2 = Config()\n",
    "config2.IMG_FOLDER = 'C:/Users/yifal/Desktop/github/Holo_Cell_Counting/data/data40x/val'\n",
    "val_dataset = HoloDataset(config2,\n",
    "                          avg,\n",
    "                          std,\n",
    "                          has_augment=transforms.Compose([ToTensor()])\n",
    "                         )\n",
    "\n",
    "if torch.cuda.is_available() & config.GPU_COUNT:\n",
    "    device = torch.device(\"cuda\", index=config.WHICH_GPU)\n",
    "else:\n",
    "    print(\"Using CPU to train the model!\")\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "net = UNet(1,2)\n",
    "net.to(device)\n",
    "cudnn.benchmark = True\n",
    "\n",
    "# training\n",
    "start_time = timer()\n",
    "loss_history,accuracy_history,dice_history,val_loss,val_accuracy,val_dice_score = train(config,net,train_dataset,val_dataset,shuffle=True)\n",
    "end_time = timer()\n",
    "print('Training Time: {}'.format(end_time - start_time))\n",
    "# print('Training Accuracy: {}'.format(accuracy_history))\n",
    "# print('Validation Accuracy: {}'.format(val_accuracy))\n",
    "\n",
    "print('The best loss in validation dataset:{}'.format(np.min(val_loss)))\n",
    "print('The best dice score in validation dataset:{}'.format(np.max(val_dice_score)))\n",
    "accur_temp = [ele[0] for ele in val_accuracy]\n",
    "print('The best accuracy in validation dataset:{}'.format(np.max(accur_temp)))\n",
    "\n",
    "\n",
    "### save the data\n",
    "# l = [1,2,3,4]\n",
    "# with open(\"test.txt\", \"wb\") as fp:   #Pickling\n",
    "#     pickle.dump(l, fp)\n",
    "# with open(\"test.txt\", \"rb\") as fp:   # Unpickling\n",
    "#     b = pickle.load(fp)\n",
    "\n",
    "\n",
    "loss_train_file = config.RESULTS_DIR + os.sep + 'loss_train_' + str(config.BASE_LR) + '_' + str(config.WEIGH_DECAY) + '.txt'\n",
    "acc_train_file = config.RESULTS_DIR + os.sep + 'acc_train_' + str(config.BASE_LR) + '_' + str(config.WEIGH_DECAY) + '.txt'\n",
    "dice_train_file = config.RESULTS_DIR + os.sep + 'dice_train_' + str(config.BASE_LR) + '_' + str(config.WEIGH_DECAY) + '.txt'\n",
    "\n",
    "loss_val_file = config.RESULTS_DIR + os.sep + 'loss_val_' + str(config.BASE_LR) + '_' + str(config.WEIGH_DECAY) + '.txt'\n",
    "acc_val_file = config.RESULTS_DIR + os.sep + 'acc_val_' + str(config.BASE_LR) + '_' + str(config.WEIGH_DECAY) + '.txt'\n",
    "dice_val_file = config.RESULTS_DIR + os.sep + 'dice_val_' + str(config.BASE_LR) + '_' + str(config.WEIGH_DECAY) + '.txt'\n",
    "\n",
    "with open(loss_train_file,'wb') as fp:\n",
    "    pickle.dump(loss_history,fp)\n",
    "with open(acc_train_file,'wb') as fp:\n",
    "    pickle.dump(accuracy_history,fp)\n",
    "with open(dice_train_file,'wb') as fp:\n",
    "    pickle.dump(dice_history,fp)\n",
    "    \n",
    "with open(loss_val_file,'wb') as fp:\n",
    "    pickle.dump(val_loss,fp)\n",
    "with open(acc_val_file,'wb') as fp:\n",
    "    pickle.dump(val_accuracy,fp)\n",
    "with open(dice_val_file,'wb') as fp:\n",
    "    pickle.dump(val_dice_score,fp)\n",
    "\n",
    "\n",
    "epoch_index = np.arange(1, config.NUM_EPOCHES+1, 1)\n",
    "plt.figure()\n",
    "plt.plot(epoch_index, loss_history, 'r--')\n",
    "plt.plot(epoch_index, val_loss, 'g--')\n",
    "plt.title(\"Loss(Training-Validation)\")\n",
    "plt.gca().legend(('Loss(Training)','Loss(Validation)'))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epoch_index, dice_history, 'r--')\n",
    "plt.plot(epoch_index, val_dice_score, 'g--')\n",
    "plt.title(\"Dice Score(Training-Validation)\")\n",
    "plt.gca().legend(('Dice Score(Training)','Dice Score(Validation)'))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1) Inference for testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.IMG_FOLDER = 'C:/Users/yifal/Desktop/github/Holo_Cell_Counting/data/data40x/test'\n",
    "test_dataset = HoloDataset(config,\n",
    "                           avg,\n",
    "                           std,\n",
    "                           has_augment=transforms.Compose([ToTensor()])\n",
    "                          )\n",
    "save_results_to = config.RESULTS_DIR\n",
    "if not os.path.exists(save_results_to):\n",
    "    os.makedirs(save_results_to)\n",
    "    \n",
    "PATH = config.MODEL_DIR + os.sep + 'UNet.pth'\n",
    "net.load_state_dict(torch.load(PATH))\n",
    "infer(config,net,test_dataset,save_results_to)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20X dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.IMG_FOLDER = 'C:/Users/yifal/Desktop/github/Holo_Cell_Counting/data/data40X20X/test_20x'\n",
    "config.RESULTS_DIR = 'C:/Users/yifal/Desktop/github/Holo_Cell_Counting/results/predictedImages_20X'\n",
    "config.OUTPUT_SHAPE = (768,768) \n",
    "test_dataset = HoloDataset(config,\n",
    "                           avg,\n",
    "                           std,\n",
    "                           has_augment=False\n",
    "                          )\n",
    "save_results_to = config.RESULTS_DIR\n",
    "if not os.path.exists(save_results_to):\n",
    "    os.makedirs(save_results_to)\n",
    "    \n",
    "PATH = config.MODEL_DIR + os.sep + 'UNet.pth'\n",
    "net.load_state_dict(torch.load(PATH))\n",
    "infer(config,net,test_dataset,save_results_to)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
